{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### <h1><center>CMSC 471: Introduction to Artificial Intelligence</center></h1>\n",
    "\n",
    "<center><img src=\"img/title.jpeg\" align=\"center\"/></center>\n",
    "\n",
    "\n",
    "<h3 style=\"color:blue;\"><center>Instructor: Fereydoon Vafaei</center></h3>\n",
    "\n",
    "\n",
    "<h5 style=\"color:purple;\"><center>Informed Search - Heuristic Functions<br></center></h5>\n",
    "\n",
    "<center><img src=\"img/UMBC_logo.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Agenda</center></h1>\n",
    "\n",
    "- <b> Chapter 3: Informed Search & Heuristic Functions</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Uninformed Search Summary</center></h1>\n",
    "\n",
    ">This table is a simplified version of the table in page 84 of our texbook (Figure 3.15).  Here $b$ is the branching factor, $d$ is the depth of the shallowest solution, $m$ is the maximum depth of the search tree, and $l$ is the depth limit. \n",
    "\n",
    "|  Criterion  |  Breadth-First  |  Depth-First  |  Depth-Limited  |  Iterative-Deepening  |  Bidirectional  \n",
    "| :-: | :-: | :-: | :-: | :-: | :-:\n",
    "|  Complete?  |  Yes  |  No  |  No  |  Yes  |  Yes  |\n",
    "|  Optimal?  |  Yes  |  No  |  No  |  Yes  |  Yes  |\n",
    "|  Time  |  $O(b^d)$  |  $O(b^m)$  |  $O(b^l)$  |  $O(b^d)$  |  $O(b^{d/2})$  |\n",
    "|  Space  |  $O(b^d)$  |  $O(bm)$  |  $O(bl)$  |  $O(bd)$  |  $O(b^{d/2})$  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1><center>Uninformed Search Summary</center></h1>\n",
    "\n",
    "Our textbook authors say:\n",
    "\n",
    "> \"In general, iterative deepening is the preferred uninformed search method when the search space is large and the depth of the solution is not known.\"\n",
    "\n",
    "[Watch this short video](https://www.youtube.com/watch?v=EnX8cQPiB1M) by [Richard Korf - UCLA](https://scholar.google.com/citations?user=LsuWoRoAAAAJ&hl=en) one of the developers of iterative deepening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Informed Search</center></h1>\n",
    "\n",
    "- Informed Search uses problem-specific knowledge beyond the definition of the problem itself.\n",
    "- <b>Heuristic function $h(n)$</b>: estimated cost of the cheapest path from the state at node $n$ to a goal state - this is non-negative and problem specific.\n",
    "- <b>Best-first search</b>: A node is selected for expansion based on an **evaluation function $f(n)$**. The definition of $f(n)$ and the way it's computed differ in each algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Informed Search</center></h1>\n",
    "\n",
    "- Informed Search is also known as <font color=\"blue\">heuristic</font> search.\n",
    "\n",
    "- For instance, A* is informed by an estimate of the total path cost through each node, and the next unexpanded node with the lowest estimated cost is expanded next. The calculation is as follows:\n",
    "\n",
    "    The total path cost for the current node =\n",
    "      the sum of the step costs so far from the start node to the current node\n",
    "         +\n",
    "      an estimate of the sum of the remaining step costs from the current node to the goal\n",
    "\n",
    "<font color=\"blue\">*heuristic function*</font>: $h(n) =$ **estimated cost** of the cheapest path from state at node $n$ to a goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Evaluation Function: Step Costs and Heuristic</center></h1>\n",
    "\n",
    "- Let's label these as:\n",
    "\n",
    "   * $f(n) =$ estimated cost of the solution path through node $n$\n",
    "   * $g(n) =$ the sum of the step costs so far from the start node to this node\n",
    "   * $h(n) =$ an **estimate** of the sum of the remaining step costs from this node to a goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Uniform Cost Search is NOT an Informed Search</center></h1>\n",
    "\n",
    "-  **Uniform Cost Search** is an <font color=\"blue\"><b>Uninformed Search</b></font> algorithm because NO heuristic is used in computing the evaluation function $f(n)$\n",
    "- Expand the node $n$ with the lowest path cost.\n",
    "    - $f(n) = g(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Greedy Best-First Search is an Informed Search</center></h1>\n",
    "\n",
    "- **Greedy Best First Search** is an <font color=\"blue\"><b>Informed Search</b></font> algorithm because heuristic is used in computing the evaluation function $f(n)$\n",
    "- Expand the node that is estimated to be the closest to the goal. Thus, the evaluation function $f(n)$ is simply the heuristic function $h(n)$\n",
    "    - $f(n) = h(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/greedy.jpg\" align=\"center\"/>\n",
    "\n",
    "image from: https://slideplayer.com/slide/4318958/14/images/35/GREEDY+BEST+FIRST+SEARCH.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>A* Search</center></h1>\n",
    "\n",
    "- Expand the node that has the minimum value of $f(n)$ where\n",
    "    - $f(n) = g(n) + h(n)$\n",
    "    - $g(n)$: the cost from the start state to the current node\n",
    "    - $h(n)$: the estimated cost from the current node to the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>A* Search</center></h1>\n",
    "\n",
    "<img src=\"img/astar1.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>A* Search</center></h1>\n",
    "\n",
    "<img src=\"img/astar2.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> Admissible Heuristic Function</center></h1>\n",
    "\n",
    "- An <font color=\"blue\"><b>admissible heuristic</b></font> is one that **never overestimates** the cost of the minimum cost path from a node to the goal node.  So, a heuristic is specific to a particular state space, and also to a particular goal state in that state space.  It must be <font color=\"blue\"><b>admissible</b></font> for all states in that search space.\n",
    "\n",
    "$$\\forall\\, node\\,n, h(n) \\le h^*(n)$$\n",
    "> where $h^*(n)$ is the true actual (minimal) cost from $n$ to goal\n",
    "\n",
    "<br>\n",
    "\n",
    "- To help remember whether an admissible heuristic \"never overestimates\" or \"never underestimates\", just remember that an <font color=\"blue\"><b>admissible heuristic is always optimistic</b></font>. If the heuristic $h$ value is too high, i.e. the heuristic overestimates the cost, it may  prevent  $A^*$  from expanding a node that is on the optimal path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> Consistent Heuristic Function</center></h1>\n",
    "\n",
    "- A stronger requirement on a heuristic is that it is <font color=\"blue\"><b>consistent</b></font>, sometimes called **monotonic**.  A heuristic $h$ is consistent if its value is nondecreasing along a path. Mathematically, a heuristic $h$ is consistent if for every node $n$ of a parent node $p$,\n",
    "\n",
    "$$h(p) \\le h(n) + \\mathrm{stepcost}(p,n)$$\n",
    "\n",
    "- Every consistent heuristic must be admissible. Proving admissibility is not sufficient for proving consistency. However, showing that a heuristic is not admissible is enough to prove that it is not consistent either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> Consistent Heuristic Function</center></h1>\n",
    "\n",
    "<center><img src=\"img/fig-3-19.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> <font color=\"blue\">Active Learning</font>: A* Search Exercise-1</center></h1>\n",
    "\n",
    ">Solve [this A* search problem](A-star-Example-1.pdf). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>: A* Search Exercise-2</center></h1>\n",
    "\n",
    "> Watch this video and solve the problem once yourself:\n",
    "\n",
    "https://www.youtube.com/watch?v=6TsL96NAZCo&t=567s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Optimality of Greedy BFS and A*</center></h1>\n",
    "\n",
    "- Greedy Best First Search is neither complete nor optimal.\n",
    "\n",
    "\n",
    "- $A^*$ is complete (mathematical proof exists but not required for the exam).\n",
    "\n",
    "\n",
    "- The tree-search version of A* is optimal if $h(n)$ is admissible, while the graph-version is optimal if $h(n)$ is consistent (mathematical proof in the textbook - but not required).\n",
    "\n",
    "\n",
    "-  $A^*$ has a high space complexity. It runs out of memory pretty quickly because it expands a lot of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Weighted A*</center></h1>\n",
    "\n",
    "- $A^*$ search has many good qualities, but it expands a lot of nodes.\n",
    "\n",
    "\n",
    "- We can explore fewer nodes (taking less time and space) if we are willing to accept solutions that are suboptimal but are \"good enough\" --- what we call **satisficing solutions**.\n",
    "\n",
    "\n",
    "- **Weighted $A^*$** weights the heuristic more heavily:\n",
    "    $$ f(n) = g(n) + W \\times h(n) \\hspace{1cm} (W >1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Weighted A* Example</center></h1>\n",
    "\n",
    "<center><img src=\"img/fig-3-21.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Weighted A* Generalization</center></h1>\n",
    "\n",
    "- **Weighted $A^*$** can be seen as a generalization of other informed search algorithms:\n",
    "\n",
    "    - $A^*$ search: $g(n) + h(n) \\hspace{1cm}(W=1)$\n",
    "    \n",
    "    - Uniform-cost search: $g(n) \\hspace{1cm}(W=0)$\n",
    "    \n",
    "    - Greedy best-first search: $h(n) \\hspace{1cm}(W=\\infty)$\n",
    "    \n",
    "    - Weighted search: $g(n) + W \\times h(n) \\hspace{1cm}(1<W<\\infty)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Informed Search Comparison</center></h1>\n",
    "\n",
    "- Notice that optimality of $A^*$ depends on the admissibility of the heuristic $h$.\n",
    "\n",
    "- **Note**: Uniform-cost search is an **UNINFORMED** search algorithm because it doesn't use heuristic $h(n)$. Nevertheless, we've put it here in this table for comparison on the performance.\n",
    "\n",
    "|Algorithm|*f*|Optimality|\n",
    "|:---------|---:|:----------:|\n",
    "|Greedy best-first search | *f = h*|nonoptimal|\n",
    "|Extra weighted A* search | *f = g + 2 &times; h*|nonoptimal|\n",
    "|Weighted A* search | *f = g + 1.4 &times; h*|nonoptimal|\n",
    "|A* search | *f = g + h*|optimal|\n",
    "|Uniform-cost search | *f = g*|optimal|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> Memory-Bounded Search</center></h1>\n",
    "\n",
    "- The main issue with $A^*$ is its use of memory.\n",
    "\n",
    "\n",
    "- No other algorithm that extends search paths from the start node and uses the same heuristic information will expand fewer nodes that $A^*$.\n",
    "\n",
    "\n",
    "- However, maintaining the list of unexpanded frontier nodes can quickly consume all storage.  This is why sometimes the  modified versions of $A^*$ are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> Memory-Bounded Search - Beam Search</center></h1>\n",
    "\n",
    "- **Beam Search** limits the size of frontier. The easiest approach is to keep only the $k$ nodes with the best $f$-scores, discarding any other expanded nodes.\n",
    "\n",
    "\n",
    "- This makes the **Beam Search** incomplete and suboptimal, but we can choose $k$ to make good use of available memory, and the algorithm executes fast because it expands fewer nodes, and for many problems it can find near-optimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Search Contours</center></h1>\n",
    "\n",
    "- You can think of uniform-cost search or $A^*$ search as spreading out everywhere in cocentric contours, and think of **Beam Search** as exploring only a focused portion of those contours, the portion that contains the $k$ best candidates.\n",
    "\n",
    "<center><img src=\"img/fig-3-20.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> Memory-Bounded Search - RBFS</center></h1>\n",
    "\n",
    "- Recursive-best-first-search **RBFS** strategy throws away and regenerates nodes and reduces the maximum number of nodes stored at any point of the algorithm.  Its space complexity is linear in the depth of the deepest optimal solution. Its time complexity is hard to characterize as it depends on the accuracy of the heuristic function.\n",
    "\n",
    "\n",
    "- Recursive Best First Search (RBFS) - uses only linear space with a DFS strategy with a f-limit. Once the current node exceeds the limit, the recursion unwinds back to the alternative path and replaces the f-value of each node on the path with the best f-value of its children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center> Memory-Bounded Search - SMA* & IDA*</center></h1>\n",
    "\n",
    "- RBFS throws away too many nodes to be as efficient in time as it can be.  Alternatives include the simplified memory-bounded A\\*, SM$A^*$ algorithm.  SM$A^*$ proceeds like a graph-based search maintaining the unexplored frontier list.  When it runs out of memory, it deletes the node with the worst $f$ value and backs that value up to the deleted node's parent.\n",
    "\n",
    "\n",
    "- Simplified MA* (SMA*) - like $A^*$ expands the best leaf until memory is full, then drops the worst leaf node, also backs up the value of the forgotten node to its parent.\n",
    "\n",
    "\n",
    "- Iterative Deepening $A^*$ (ID$A^*$) - uses f-cost, i.e. $(g+h)$ rather than the depth for limit. At each iteration, the cutoff value is the smallest f-cost of any node that exceeded the cutoff on the previous iteration.\n",
    "\n",
    "\n",
    "- Iterative Deepening $A^*$ (ID$A^*$) is to $A^*$ what iterative-deepening search is to depth-first: (ID$A^*$) gives us the benfits of without the requirement to keep all reached states in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Heuristic Functions</center></h1>\n",
    "\n",
    "- Using a good heuristic is important in determining the performance of $A^*$. The value of $h(n)$ would ideally equal the exact cost of reaching the destination. This is, however, not possible because we do not even know the path.\n",
    "\n",
    "\n",
    "- Also, for some problems sometimes we may want to use combination of heuristics.\n",
    "\n",
    "\n",
    "- As an example, if $h_1(n)$ and $h_2(n)$ are both admissible, is $max\\{h_1(n), h_2(n)\\}$ admissible? How about $min\\{h_1(n), h_2(n)\\}$? How about $h_1(n) + h_2(n)$ ?\n",
    "\n",
    "https://brilliant.org/wiki/a-star-search/\n",
    "\n",
    "http://theory.stanford.edu/~amitp/GameProgramming/Heuristics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Effective Branching Factor</center></h1>\n",
    "\n",
    "- One way to charachterize the quality of a heuristic is the **effective branching factor** $b^*$\n",
    "\n",
    "- If the total number of nodes generated by A* for a particular problem is $N$ and the solution depth is $d$, then $b*$ is the branching factor that a uniform tree of depth $d$ would have to have in order to contain $N+1$ nodes.\n",
    "\n",
    "- Thus, $N+1=1+b^*+{(b^*)}^2+...+{(b^*)}^d$\n",
    "\n",
    "- For example, if A* finds a solution at depth 5 using 52 nodes, then the effective branching factor is 1.92\n",
    "\n",
    "- The effective branching factor can vary across problem instances, but usually for a specific domain (such as 8-puzzles) it is fairly constant across all nontrivial problem instances.\n",
    "\n",
    "- Therefore, experimental measurements of $b^*$ on a small set of problems can provide a good guide to the heuristic's overall usefulness. \n",
    "\n",
    "- A well-designed heuristic would have a value of $b^*$ close to 1, allowing fairly large problems to be solved at reasonable computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>The Effect of Heuristics</center></h1>\n",
    "\n",
    "- For any node $n$, $h_2(n) \\ge h_1(n)$ thus we say that $h_2$ **dominates** $h_1$\n",
    "- Domination translates directly into efficiency: A* using $h_2$ will NEVER expand more nodes than A* using $h_1$ and therefore $h_2$ is better than $h_1$\n",
    "\n",
    "<center><img src=\"img/fig-3-26.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Generating Heuristics from Relaxed Problems and Subproblems</center></h1>\n",
    "\n",
    "- A problem with fewer restrictions on the actions is called a **relaxed problem**.\n",
    "\n",
    "- The cost of an optimal solution to a relaxed problem is an admissible heuristic for the original problem.\n",
    "\n",
    "<center><img src=\"img/fig-3-27.png\" align=\"center\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1><center>Summary of Chapter 3</center></h1>\n",
    "\n",
    "- Search environment characteristics: Observable, Known, Discrete, Deterministic\n",
    "\n",
    "- <font color=\"blue\">Uninformed Search</font>: BFS, Uniform-Cost Search, DFS, Depth-limited Search, Iterative Deepening Search, Bidirectional Search \n",
    "\n",
    "- <font color=\"blue\">Informed Search</font>: Greedy Best First Search, $A^*$ Search, Weighted $A^*$ Search, Beam Search, RBFS, SM$A^*$, ID$A^*$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
